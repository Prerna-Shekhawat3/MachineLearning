# -*- coding: utf-8 -*-
"""Day2 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I30hEoADWU79zpQ7bdoXSZY4srk74I4O

#Working with CSV Files
"""

import pandas as pd
import numpy as np

"""#Reading downloded CSV file"""

df=pd.read_csv('/content/cars.csv')

"""#Reading from url"""

import requests
from io import StringIO

url=''
header={'User-Agent':"Mozilla/5.0"}
req=requests.get(url,headers=header)
data=StringIO(req.text)

pd.read_csv(data)

dff=pd.read_csv('/content/movie_titles_metadata.tsv',sep='\t')
dff.head(5)

"""#1)sep , names
##      Column names are not specified so we will use 'names' parameter of read_csv() function in order to define the names

## There can be many types of separators ->
### 1) ,(default)
### 2)';' 'tab'->sep='\t'
### 3)';'->sep=';'


"""

dff=pd.read_csv('/content/movie_titles_metadata.tsv',sep='\t',names=['serno','name','release year','ratings','votes','genres'])
dff.head(5)

"""#2)index_cols=' '
## When we want to change the default index to a particular column we use index_col() parameter
"""

dff=pd.read_csv('/content/movie_titles_metadata.tsv',sep='\t',index_col='serno',names=['serno','name','release year','ratings','votes','genres'])
dff.head(5)

"""#3)header=1

## Sometimes the column name is also taken as row so we need to make it back column header then we need to use header=1
"""

df3=pd.read_csv('/content/test.csv')
df3.head()

pd.read_csv('/content/test.csv',header=1)

"""#4)usecols=[ ] and usecols=[ ],squeeze=True


##usecols=[ ],squeeze=True-> When we want only one column

## Many often we are given a large number of features of which not many are requires so we use usecols parameter to specifiv the columns
"""

pd.read_csv('/content/test.csv',header=1,usecols=['enrollee_id','gender'])

"""#5) Skiprows/nrows

##When we want to skip rows and nrow when we want only particular number of rows from dataset
"""

pd.read_csv('/content/cars.csv',nrows=1000)

"""##6) Encoding Parameters"""

pd.read_csv('/content/zomato.csv',encoding='latin-1')

"""#7) Parser Error- error_bad_lines=False

##When there are 5 columns but have 6 values at that time pandas will not be an=ble to read CSV file so we use error_bad_lines parameter
"""

pd.read('file_name',error_bad_lines=False)

"""#8) dtype

##It is uesd to change the data type of columns
"""

df=pd.read_csv('/content/aug_train.csv')
df.sample(3)

pd.read_csv('/content/aug_train.csv',dtype={'target':int}).info()

df['target'].astype(int)

"""#9)Handling Dates

##parse_dates=[column_names]
"""

pd.read_csv('/content/IPL Matches 2008-2020.csv').info()

"""##Here the dates column is identified as String so we need to convert it into date format"""

pd.read_csv('/content/IPL Matches 2008-2020.csv',parse_dates=['date']).info()

"""#10)Converters

#They are used when we want to apply python function on dateset columns
"""

pd.read_csv('/content/IPL Matches 2008-2020.csv').head(2)

def rename(name):

  res = ""
  for i in range(len(name)):
    if i == 0:
      res += name[i].upper()
    elif name[i] == " ":
      res += name[i + 1].upper()
  return res

pd.read_csv('/content/IPL Matches 2008-2020.csv',converters={'team1':rename}).head(3)

"""#11) na_values: This is used when we want to consider a set of values as Nan

##In some case the missing values are not specified as Nan but [- , % ] or any other such value is used so we can consider them as Nan values
"""

pd.read_csv('',na_values=['-'])

"""#12) Loading data set into chunks

##Many times when we have big dataset we can divide it into chunks by specifiying chunksize
"""

pd.read_csv('/content/aug_train.csv').shape

df=pd.read_csv('/content/aug_train.csv',chunksize=5000)

for i in df:
  print(i.shape)

